{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22144b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-16 10:32:37.951765: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-16 10:32:37.973938: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747405957.997398    4151 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747405958.004275    4151 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747405958.023881    4151 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747405958.023947    4151 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747405958.023950    4151 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747405958.023952    4151 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-16 10:32:38.031817: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from ai_edge_litert.interpreter import Interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e498d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = \"/home/dylenthomas/whisperProject/whisper/.model/WhisperEncoder.tflite\"\n",
    "decoder = \"/home/dylenthomas/whisperProject/whisper/.model/WhisperDecoder.tflite\"\n",
    "audio = \"/home/dylenthomas/whisperProject/whisper/8455-210777-0068.wav\"\n",
    "full_model = \"/home/dylenthomas/whisperProject/whisper/.model/whisper-small.en.tflite\"\n",
    "\n",
    "def print_details(details, type):\n",
    "    print(\"-\"*10 + f\" {type} details \".upper() + \"-\"*10)\n",
    "    for detail in details:\n",
    "        print(detail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f3c1682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- TEST INPUT DETAILS ----------\n",
      "{'name': 'serving_default_input_features:0', 'index': 0, 'shape': array([   1,   80, 3000], dtype=int32), 'shape_signature': array([   1,   80, 3000], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
      "---------- TEST OUTPUT DETAILS ----------\n",
      "{'name': 'StatefulPartitionedCall:0', 'index': 1838, 'shape': array([  1, 449], dtype=int32), 'shape_signature': array([  1, 449], dtype=int32), 'dtype': <class 'numpy.int32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n"
     ]
    }
   ],
   "source": [
    "test = Interpreter(model_path=full_model)\n",
    "\n",
    "test_input_details = test.get_input_details()\n",
    "test_output_details = test.get_output_details()\n",
    "\n",
    "print_details(test_input_details, \"test input\")\n",
    "print_details(test_output_details, \"test output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ef9f9438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.1483, -0.0561,  0.0037,  ..., -0.6740, -0.6740, -0.6740],\n",
      "         [-0.2433,  0.0427, -0.0107,  ..., -0.6740, -0.6740, -0.6740],\n",
      "         [ 0.0475,  0.0221,  0.0333,  ..., -0.6740, -0.6740, -0.6740],\n",
      "         ...,\n",
      "         [-0.6740, -0.6740, -0.6740,  ..., -0.6740, -0.6740, -0.6740],\n",
      "         [-0.6740, -0.6740, -0.6740,  ..., -0.6740, -0.6740, -0.6740],\n",
      "         [-0.6740, -0.6740, -0.6740,  ..., -0.6740, -0.6740, -0.6740]]])\n"
     ]
    }
   ],
   "source": [
    "from utils.WhisperProcessor import offlineWhisperProcessor\n",
    "import torch\n",
    "\n",
    "signal, _ = librosa.load(audio, sr=16000, mono=True)\n",
    "\n",
    "processor = offlineWhisperProcessor(\"/home/dylenthomas/whisperProject/whisper/utils/preprocessor_config.json\",\n",
    "                                    \"/home/dylenthomas/whisperProject/whisper/utils/tokenizer_config.json\",\n",
    "                                    \"/home/dylenthomas/whisperProject/whisper/utils/vocab.json\")\n",
    "signal_test = torch.Tensor(signal)\n",
    "\n",
    "waveform_features = processor.extract_features(signal_test)\n",
    "print((waveform_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8f85f0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.allocate_tensors()\n",
    "test.set_tensor(test_input_details[0][\"index\"], waveform_features)\n",
    "\n",
    "with tf.device(\"/CPU:0\"):\n",
    "    test.invoke()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c8f2b5c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50257 50363  3406  1176   318  6751    11   314   531    13 50469 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256 50256\n",
      " 50256 50256 50256 50256 50256]\n",
      "<|endoftext|>\n",
      "<|notimestamps|>\n",
      "Ġom\n",
      "ĠZ\n",
      "ĠS\n",
      "Ġexperienced\n",
      ",\n",
      "ĠT\n",
      "Ġë\n",
      ".\n",
      "<|2.10|>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = test.get_tensor(test_output_details[0][\"index\"])[0, :]\n",
    "\n",
    "print(output)\n",
    "for tok in output:\n",
    "    print(processor.decode_single(tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53945512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- ENCODER INPUT DETAILS ----------\n",
      "{'name': 'audio', 'index': 0, 'shape': array([   1,   80, 3000], dtype=int32), 'shape_signature': array([   1,   80, 3000], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
      "---------- ENCODER OUTPUT DETAILS ----------\n",
      "{'name': 'k_cache', 'index': 387, 'shape': array([   4,    6,   64, 1500], dtype=int32), 'shape_signature': array([   4,    6,   64, 1500], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
      "{'name': 'v_cache', 'index': 420, 'shape': array([   4,    6, 1500,   64], dtype=int32), 'shape_signature': array([   4,    6, 1500,   64], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
      "---------- DECODER INPUT DETAILS ----------\n",
      "{'name': 'x', 'index': 0, 'shape': array([1, 1], dtype=int32), 'shape_signature': array([1, 1], dtype=int32), 'dtype': <class 'numpy.int32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
      "{'name': 'index', 'index': 1, 'shape': array([1, 1], dtype=int32), 'shape_signature': array([1, 1], dtype=int32), 'dtype': <class 'numpy.int32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
      "{'name': 'k_cache_cross', 'index': 2, 'shape': array([   4,    6,   64, 1500], dtype=int32), 'shape_signature': array([   4,    6,   64, 1500], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
      "{'name': 'v_cache_cross', 'index': 3, 'shape': array([   4,    6, 1500,   64], dtype=int32), 'shape_signature': array([   4,    6, 1500,   64], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
      "{'name': 'k_cache_self', 'index': 4, 'shape': array([  4,   6,  64, 224], dtype=int32), 'shape_signature': array([  4,   6,  64, 224], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
      "{'name': 'v_cache_self', 'index': 5, 'shape': array([  4,   6, 224,  64], dtype=int32), 'shape_signature': array([  4,   6, 224,  64], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
      "---------- DECODER OUTPUT DETAILS ----------\n",
      "{'name': 'logits', 'index': 685, 'shape': array([    1,     1, 51864], dtype=int32), 'shape_signature': array([    1,     1, 51864], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
      "{'name': 'k_cache', 'index': 575, 'shape': array([  4,   6,  64, 224], dtype=int32), 'shape_signature': array([  4,   6,  64, 224], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
      "{'name': 'v_cache', 'index': 604, 'shape': array([  4,   6, 224,  64], dtype=int32), 'shape_signature': array([  4,   6, 224,  64], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n"
     ]
    }
   ],
   "source": [
    "encoder_interpreter = Interpreter(model_path=encoder)\n",
    "decoder_interpreter = Interpreter(model_path=decoder)\n",
    "\n",
    "encoder_input_details = encoder_interpreter.get_input_details()\n",
    "encoder_output_details = encoder_interpreter.get_output_details()\n",
    "decoder_input_details = decoder_interpreter.get_input_details()\n",
    "decoder_output_details = decoder_interpreter.get_output_details()\n",
    "\n",
    "\n",
    "\n",
    "print_details(encoder_input_details, \"encoder input\")\n",
    "print_details(encoder_output_details, \"encoder output\")\n",
    "print_details(decoder_input_details, \"decoder input\")\n",
    "print_details(decoder_output_details, \"decoder output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64f81ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k_cache shape: (4, 6, 64, 1500)\n",
      "v_cache shape: (4, 6, 1500, 64)\n"
     ]
    }
   ],
   "source": [
    "encoder_interpreter.allocate_tensors()\n",
    "\n",
    "#set the audio tensor for inference\n",
    "encoder_interpreter.set_tensor(encoder_input_details[0][\"index\"], waveform_features)\n",
    "\n",
    "with tf.device(\"/CPU:0\"):\n",
    "    #run inference\n",
    "    encoder_interpreter.invoke()\n",
    "\n",
    "k_cache = encoder_interpreter.get_tensor(encoder_output_details[0][\"index\"])\n",
    "v_cache = encoder_interpreter.get_tensor(encoder_output_details[1][\"index\"])\n",
    "\n",
    "print(\"k_cache shape: {}\".format(k_cache.shape))\n",
    "print(\"v_cache shape: {}\".format(v_cache.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8ecc6ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "decoder_interpreter.allocate_tensors()\n",
    "\n",
    "#set the start (or last predicted token) token\n",
    "decoder_interpreter.set_tensor(decoder_input_details[0][\"index\"], np.array([[50258]], np.int32))\n",
    "#set the starting index\n",
    "decoder_interpreter.set_tensor(decoder_input_details[1][\"index\"], np.array([[0]], np.int32)) # seems to like to start at 1\n",
    "#set the k and v caches for decoding\n",
    "decoder_interpreter.set_tensor(decoder_input_details[2][\"index\"], k_cache)\n",
    "decoder_interpreter.set_tensor(decoder_input_details[3][\"index\"], v_cache)\n",
    "#initialize self attn\n",
    "decoder_interpreter.set_tensor(decoder_input_details[4][\"index\"], np.zeros((4, 6, 64, 224), np.float32))\n",
    "decoder_interpreter.set_tensor(decoder_input_details[5][\"index\"], np.zeros((4, 6, 224, 64), np.float32))\n",
    "#run inference\n",
    "decoder_interpreter.invoke()\n",
    "\n",
    "logits = decoder_interpreter.get_tensor(decoder_output_details[0][\"index\"])\n",
    "prev_k_cache = decoder_interpreter.get_tensor(decoder_output_details[1][\"index\"])\n",
    "prev_v_cache = decoder_interpreter.get_tensor(decoder_output_details[2][\"index\"])\n",
    "\n",
    "probabilities = tf.nn.softmax(logits, axis=-1)\n",
    "val = tf.argmax(probabilities, axis=-1).numpy()\n",
    "val = val.item()\n",
    "print(processor.decode_single(val))\n",
    "\n",
    "# I think the pipeline for this model is you feed the token that came out of the decoder aswell as the encoded signal back in to the decoder (without encoding agian) with self attention until it hits the end of transcript token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56eb69e",
   "metadata": {},
   "source": [
    "### Find another tflite encoder decoder file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31aa5a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===Start Token===\n",
      "<|startoftranscript|> 50258\n",
      "======\n",
      "k_cache mean/std: 0.00564328 0.2848788\n",
      "v_cache mean/std: 0.010671045 0.34908122\n",
      "<|endoftext|> 50257\n"
     ]
    }
   ],
   "source": [
    "not_end = True\n",
    "i = 0\n",
    "token = 50258 #<|startoftranscript|>\n",
    "print(\"===Start Token===\")\n",
    "print(processor.decode_single(token), token)\n",
    "print(\"=\"*6)\n",
    "\n",
    "decoder_interpreter.allocate_tensors()\n",
    "\n",
    "prev_k_cache = np.zeros((4, 6, 64, 224), np.float32)\n",
    "prev_v_cache = np.zeros((4, 6, 224, 64), np.float32)\n",
    "\n",
    "while not_end:\n",
    "    decoder_interpreter.set_tensor(decoder_input_details[0][\"index\"], np.array([[token]], np.int32))\n",
    "    decoder_interpreter.set_tensor(decoder_input_details[1][\"index\"], np.array([[i]], np.int32))\n",
    "    decoder_interpreter.set_tensor(decoder_input_details[2][\"index\"], k_cache)\n",
    "    decoder_interpreter.set_tensor(decoder_input_details[3][\"index\"], v_cache)\n",
    "    decoder_interpreter.set_tensor(decoder_input_details[4][\"index\"], prev_k_cache)\n",
    "    decoder_interpreter.set_tensor(decoder_input_details[5][\"index\"], prev_v_cache)\n",
    "\n",
    "    with tf.device(\"/CPU:0\"):\n",
    "        #inference\n",
    "        decoder_interpreter.invoke()\n",
    "\n",
    "    logits = decoder_interpreter.get_tensor(decoder_output_details[0][\"index\"])\n",
    "    prev_k_cache = decoder_interpreter.get_tensor(decoder_output_details[1][\"index\"])\n",
    "    prev_v_cache = decoder_interpreter.get_tensor(decoder_output_details[2][\"index\"])\n",
    "\n",
    "    #probabilities = tf.nn.softmax(logits, axis=-1)\n",
    "    #token = tf.argmax(probabilities, axis=-1).numpy().item()\n",
    "    token = np.argmax(logits[0, -1])\n",
    "    print(processor.decode_single(token), token)\n",
    "\n",
    "    if token == 50257:\n",
    "        not_end = False\n",
    "\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf5fee0",
   "metadata": {},
   "source": [
    "For now I can deploy using this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3e021db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny\")\n",
    "\n",
    "import os\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5405c0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ĠYour\n",
      "Ġpower\n",
      "Ġis\n",
      "Ġsufficient\n",
      ",\n",
      "ĠI\n",
      "Ġsaid\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "predicted = model.generate(waveform_features)[0]\n",
    "\n",
    "for tok in predicted:\n",
    "    print(processor.decode_single(tok))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
